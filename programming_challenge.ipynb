{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 116351,
          "databundleVersionId": 13878378,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "testone1",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LoPA607/IE643/blob/main/programming_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "087l2mFfAoCS"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "ie_643_2025_programming_challenge_1_path = kagglehub.competition_download('ie-643-2025-programming-challenge-1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "_BZv9TnGAoCV"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-28T05:53:13.884256Z",
          "iopub.execute_input": "2025-09-28T05:53:13.884656Z",
          "iopub.status.idle": "2025-09-28T05:53:16.999599Z",
          "shell.execute_reply.started": "2025-09-28T05:53:13.884622Z",
          "shell.execute_reply": "2025-09-28T05:53:16.998477Z"
        },
        "id": "unjxhjNtAoCW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "# ================================\n",
        "# Load Full Data\n",
        "# ================================\n",
        "print(\"Loading Train Data & Labels...\")\n",
        "train_data = pd.read_csv(\"/kaggle/input/ie-643-2025-programming-challenge-1/Train_Data.csv\")        # (68599, 1024)\n",
        "train_labels = pd.read_csv(\"/kaggle/input/ie-643-2025-programming-challenge-1/Train_Labels.csv\")    # (68599, 25)\n",
        "print(\"Train Data:\", train_data.shape)\n",
        "print(\"Train Labels:\", train_labels.shape)\n",
        "\n",
        "print(\"Loading Test Data...\")\n",
        "test_data = pd.read_csv(\"/kaggle/input/ie-643-2025-programming-challenge-1/Test_Data.csv\")          # (17150, 1025: ID + features)\n",
        "print(\"Test Data:\", test_data.shape)\n",
        "\n",
        "# Extract IDs and features\n",
        "test_ids = test_data[\"ID\"].values\n",
        "X_test = test_data.drop(columns=[\"ID\"]).values.astype(np.float32)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-28T06:33:51.196828Z",
          "iopub.execute_input": "2025-09-28T06:33:51.197186Z",
          "iopub.status.idle": "2025-09-28T06:34:08.196626Z",
          "shell.execute_reply.started": "2025-09-28T06:33:51.197163Z",
          "shell.execute_reply": "2025-09-28T06:34:08.195025Z"
        },
        "id": "E3jM9q6uAoCW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.ensemble import IsolationForest"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-28T06:33:24.815212Z",
          "iopub.execute_input": "2025-09-28T06:33:24.815574Z",
          "iopub.status.idle": "2025-09-28T06:33:24.82113Z",
          "shell.execute_reply.started": "2025-09-28T06:33:24.815549Z",
          "shell.execute_reply": "2025-09-28T06:33:24.819984Z"
        },
        "id": "E5OF_xb7AoCX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X = train_data.values.astype(np.float32)\n",
        "y = train_labels.values.astype(np.float32)\n",
        "\n",
        "# --- Step 1: Reduce dimensionality for anomaly detection\n",
        "pca = PCA(n_components=50, random_state=42)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "\n",
        "# --- Step 2: Detect anomalies (20% assumed noisy)\n",
        "iso = IsolationForest(contamination=0.2, random_state=42, n_jobs=-1)\n",
        "outlier_flags = iso.fit_predict(X_reduced)  # -1 = noisy, 1 = clean\n",
        "\n",
        "# --- Step 3: Keep only clean samples\n",
        "clean_idx = np.where(outlier_flags == 1)[0]\n",
        "X_clean, y_clean = X[clean_idx], y[clean_idx]\n",
        "\n",
        "print(\"Original Train Samples:\", X.shape[0])\n",
        "print(\"Removed Noisy Samples:\", len(X) - len(X_clean))\n",
        "print(\"Remaining Clean Samples:\", X_clean.shape[0])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-28T06:33:24.822689Z",
          "iopub.execute_input": "2025-09-28T06:33:24.823048Z",
          "iopub.status.idle": "2025-09-28T06:33:34.819866Z",
          "shell.execute_reply.started": "2025-09-28T06:33:24.823017Z",
          "shell.execute_reply": "2025-09-28T06:33:34.818627Z"
        },
        "id": "SsQ-_oi2AoCX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def make_features(X_in):\n",
        "    feats = []\n",
        "\n",
        "    # PCA features\n",
        "    pca = PCA(n_components=50, random_state=42)\n",
        "    pca_feats = pca.fit_transform(X_in)\n",
        "    feats.append(pca_feats)\n",
        "\n",
        "    # statistical features\n",
        "    mean_ = X_in.mean(axis=1, keepdims=True)\n",
        "    std_ = X_in.std(axis=1, keepdims=True)\n",
        "    min_ = X_in.min(axis=1, keepdims=True)\n",
        "    max_ = X_in.max(axis=1, keepdims=True)\n",
        "    skew_ = skew(X_in, axis=1).reshape(-1,1)\n",
        "    kurt_ = kurtosis(X_in, axis=1).reshape(-1,1)\n",
        "    stats = np.hstack([mean_, std_, min_, max_, skew_, kurt_])\n",
        "    feats.append(stats)\n",
        "\n",
        "    # anomaly score feature\n",
        "    iso2 = IsolationForest(contamination=0.1, random_state=0, n_jobs=-1)\n",
        "    scores = -iso2.fit_predict(X_in)   # 1 normal, -1 anomaly\n",
        "    scores = scores.reshape(-1,1)\n",
        "    feats.append(scores)\n",
        "\n",
        "    return np.hstack(feats), pca, iso2\n",
        "X_feat, pca, iso2 = make_features(X_clean)\n",
        "\n",
        "# transform test data with same pca & iso2\n",
        "pca_feats_test = pca.transform(X_test)\n",
        "stats_test = np.hstack([\n",
        "    X_test.mean(axis=1, keepdims=True),\n",
        "    X_test.std(axis=1, keepdims=True),\n",
        "    X_test.min(axis=1, keepdims=True),\n",
        "    X_test.max(axis=1, keepdims=True),\n",
        "    skew(X_test, axis=1).reshape(-1,1),\n",
        "    kurtosis(X_test, axis=1).reshape(-1,1)\n",
        "])\n",
        "scores_test = -iso2.fit_predict(X_test).reshape(-1,1)\n",
        "\n",
        "X_test_feat = np.hstack([pca_feats_test, stats_test, scores_test])\n",
        "\n",
        "print(\"Engineered Train Features:\", X_feat.shape)\n",
        "print(\"Engineered Test Features :\", X_test_feat.shape)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-28T06:34:47.002237Z",
          "iopub.execute_input": "2025-09-28T06:34:47.002615Z",
          "iopub.status.idle": "2025-09-28T06:35:16.931522Z",
          "shell.execute_reply.started": "2025-09-28T06:34:47.002591Z",
          "shell.execute_reply": "2025-09-28T06:35:16.930475Z"
        },
        "id": "SncSaaEOAoCY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X_feat, y_clean, test_size=0.15, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val   = scaler.transform(X_val)\n",
        "X_test_feat = scaler.transform(X_test_feat)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-28T06:35:20.591483Z",
          "iopub.execute_input": "2025-09-28T06:35:20.591819Z",
          "iopub.status.idle": "2025-09-28T06:35:20.677622Z",
          "shell.execute_reply.started": "2025-09-28T06:35:20.591796Z",
          "shell.execute_reply": "2025-09-28T06:35:20.676309Z"
        },
        "id": "_jDFENG8AoCY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Preprocessing\n",
        "# ================================\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_clean, y_clean, test_size=0.15, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_val   = scaler.transform(X_val)\n",
        "X_test  = scaler.transform(X_test)\n",
        "\n",
        "# ================================\n",
        "# Torch Dataset\n",
        "# ================================\n",
        "class MultiLabelDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.X = torch.from_numpy(X).float()\n",
        "        self.y = None if y is None else torch.from_numpy(y).float()\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is None:\n",
        "            return self.X[idx]\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "train_ds = MultiLabelDataset(X_train, y_train)\n",
        "val_ds   = MultiLabelDataset(X_val, y_val)\n",
        "test_ds  = MultiLabelDataset(X_test)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
        "val_loader   = DataLoader(val_ds, batch_size=256, shuffle=False)\n",
        "test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-28T06:35:22.525297Z",
          "iopub.execute_input": "2025-09-28T06:35:22.525661Z",
          "iopub.status.idle": "2025-09-28T06:35:24.051916Z",
          "shell.execute_reply.started": "2025-09-28T06:35:22.525635Z",
          "shell.execute_reply": "2025-09-28T06:35:24.050166Z"
        },
        "id": "eqBzDrCvAoCY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Define MLP Model\n",
        "# ================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ================================\n",
        "# Define Deeper MLP with Skip Connections\n",
        "# ================================\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim=1024, hidden_dims=[1024, 768, 512, 384, 256, 128], output_dim=25, dropout=0.3):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dims[0])\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_dims[0])\n",
        "\n",
        "        self.fc2 = nn.Linear(hidden_dims[0], hidden_dims[1])\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_dims[1])\n",
        "\n",
        "        self.fc3 = nn.Linear(hidden_dims[1], hidden_dims[2])\n",
        "        self.bn3 = nn.BatchNorm1d(hidden_dims[2])\n",
        "\n",
        "        self.fc4 = nn.Linear(hidden_dims[2], hidden_dims[3])\n",
        "        self.bn4 = nn.BatchNorm1d(hidden_dims[3])\n",
        "\n",
        "        self.fc5 = nn.Linear(hidden_dims[3] + hidden_dims[1], hidden_dims[4])  # concat skip from layer2\n",
        "        self.bn5 = nn.BatchNorm1d(hidden_dims[4])\n",
        "\n",
        "        self.fc6 = nn.Linear(hidden_dims[4] + hidden_dims[0], hidden_dims[5])  # concat skip from layer1\n",
        "        self.bn6 = nn.BatchNorm1d(hidden_dims[5])\n",
        "\n",
        "        self.out = nn.Linear(hidden_dims[5], output_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h1 = F.relu(self.bn1(self.fc1(x)))\n",
        "        h1 = self.dropout(h1)\n",
        "\n",
        "        h2 = F.relu(self.bn2(self.fc2(h1)))\n",
        "        h2 = self.dropout(h2)\n",
        "\n",
        "        h3 = F.relu(self.bn3(self.fc3(h2)))\n",
        "        h3 = self.dropout(h3)\n",
        "\n",
        "        h4 = F.relu(self.bn4(self.fc4(h3)))\n",
        "        h4 = self.dropout(h4)\n",
        "\n",
        "        # skip connections: concat features from earlier layers\n",
        "        h5_in = torch.cat([h4, h2], dim=1)\n",
        "        h5 = F.relu(self.bn5(self.fc5(h5_in)))\n",
        "        h5 = self.dropout(h5)\n",
        "\n",
        "        h6_in = torch.cat([h5, h1], dim=1)\n",
        "        h6 = F.relu(self.bn6(self.fc6(h6_in)))\n",
        "        h6 = self.dropout(h6)\n",
        "\n",
        "        out = self.out(h6)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = MLP(input_dim=X_train.shape[1]).to(device)\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "\n",
        "# CosineAnnealingLR with 10 epochs per cycle\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "scheduler = CosineAnnealingLR(optimizer, T_max=10)\n",
        "\n",
        "# ================================\n",
        "# Training Loop\n",
        "# ================================\n",
        "def evaluate(model, loader):\n",
        "    model.eval()\n",
        "    ys, preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in loader:\n",
        "            xb = xb.to(device)\n",
        "            out = model(xb)\n",
        "            prob = torch.sigmoid(out).cpu().numpy()\n",
        "            ys.append(yb.numpy())\n",
        "            preds.append(prob)\n",
        "    ys = np.vstack(ys)\n",
        "    preds = np.vstack(preds)\n",
        "    aucs = []\n",
        "    for i in range(ys.shape[1]):\n",
        "        try:\n",
        "            auc = roc_auc_score(ys[:,i], preds[:,i])\n",
        "        except ValueError:\n",
        "            auc = np.nan\n",
        "        aucs.append(auc)\n",
        "    return np.nanmean(aucs)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-28T06:35:32.896061Z",
          "iopub.execute_input": "2025-09-28T06:35:32.89642Z",
          "iopub.status.idle": "2025-09-28T06:35:32.944237Z",
          "shell.execute_reply.started": "2025-09-28T06:35:32.896397Z",
          "shell.execute_reply": "2025-09-28T06:35:32.943144Z"
        },
        "id": "SNF6DMqkAoCZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "for epoch in range(1, EPOCHS+1):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    loop = tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
        "    for xb, yb in loop:\n",
        "        xb, yb = xb.to(device), yb.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        out = model(xb)\n",
        "        loss = criterion(out, yb)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item() * xb.size(0)\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    ys, preds = [], []\n",
        "    with torch.no_grad():\n",
        "        for xb, yb in val_loader:\n",
        "            xb, yb = xb.to(device), yb.to(device)\n",
        "            out = model(xb)\n",
        "            prob = torch.sigmoid(out).cpu().numpy()\n",
        "            loss = criterion(out, yb)\n",
        "            val_loss += loss.item() * xb.size(0)\n",
        "            ys.append(yb.cpu().numpy())\n",
        "            preds.append(prob)\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    ys = np.vstack(ys)\n",
        "    preds = np.vstack(preds)\n",
        "\n",
        "    # compute ROC-AUC\n",
        "    aucs = []\n",
        "    for i in range(ys.shape[1]):\n",
        "        try:\n",
        "            auc = roc_auc_score(ys[:, i], preds[:, i])\n",
        "        except ValueError:\n",
        "            auc = np.nan\n",
        "        aucs.append(auc)\n",
        "    val_auc = np.nanmean(aucs)\n",
        "\n",
        "    # step scheduler (after each epoch)\n",
        "    scheduler.step()\n",
        "\n",
        "    print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, \"\n",
        "          f\"Val Loss={val_loss:.4f}, Val ROC-AUC={val_auc:.4f}, \"\n",
        "          f\"LR={optimizer.param_groups[0]['lr']:.6f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-28T06:35:37.467284Z",
          "iopub.execute_input": "2025-09-28T06:35:37.46766Z",
          "iopub.status.idle": "2025-09-28T06:38:25.829421Z",
          "shell.execute_reply.started": "2025-09-28T06:35:37.467636Z",
          "shell.execute_reply": "2025-09-28T06:38:25.82843Z"
        },
        "id": "y3998UKdAoCa"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================\n",
        "# Inference on Test Data\n",
        "# ================================\n",
        "model.eval()\n",
        "all_preds = []\n",
        "with torch.no_grad():\n",
        "    for xb in test_loader:\n",
        "        xb = xb.to(device)\n",
        "        out = model(xb)\n",
        "        prob = torch.sigmoid(out).cpu().numpy()\n",
        "        all_preds.append(prob)\n",
        "\n",
        "all_preds = np.vstack(all_preds)   # shape: (17150, 25)\n",
        "\n",
        "# Apply threshold\n",
        "final_labels = (all_preds > 0.5).astype(int)\n",
        "\n",
        "# ================================\n",
        "# Build Submission File\n",
        "# ================================\n",
        "label_cols = train_labels.columns.tolist()  # ['A','B',...,'Y']\n",
        "submission = pd.DataFrame(final_labels, columns=label_cols)\n",
        "submission.insert(0, \"ID\", test_ids)\n",
        "\n",
        "# Save CSV (Kaggle kernel environment → must use /kaggle/working)\n",
        "submission_path = \"/kaggle/working/submission5.csv\"\n",
        "submission.to_csv(submission_path, index=False)\n",
        "\n",
        "print(submission.head())\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-09-28T06:38:26.959628Z",
          "iopub.execute_input": "2025-09-28T06:38:26.959968Z",
          "iopub.status.idle": "2025-09-28T06:38:28.387686Z",
          "shell.execute_reply.started": "2025-09-28T06:38:26.959943Z",
          "shell.execute_reply": "2025-09-28T06:38:28.386716Z"
        },
        "id": "-KuiBXDWAoCb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "HtkNhfwrAoCb"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}